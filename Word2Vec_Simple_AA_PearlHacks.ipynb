{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Workshop on Extracting Embeddings for Pearl Hacks 2024\n",
        "In this tutorial, we download the IMDb dataset and explore how to create word embeddings on the IMDb dataset reviews. We label the tokens from the word sentiments with 'pos' or 'neg'."
      ],
      "metadata": {
        "id": "YaEWmSGebS_i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "p_v3YPtMWqoW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd59612-af1c-4575-ea3a-0a4ece7a7ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import os\n",
        "from gensim.models import Word2Vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we download the IMDb dataset and extract it."
      ],
      "metadata": {
        "id": "4pS-xtMAdO8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the IMDb dataset\n",
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "dataset_file = \"aclImdb_v1.tar.gz\"\n",
        "if not os.path.exists(dataset_file):\n",
        "    urllib.request.urlretrieve(url, dataset_file)\n",
        "\n",
        "# Extract the dataset\n",
        "with tarfile.open(dataset_file, \"r:gz\") as tar:\n",
        "    tar.extractall()"
      ],
      "metadata": {
        "id": "Ozf_Kiv6dLOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, one of the most important parts in embeddings, we beginning pre processing the data. Models are trained on the preprocessed data. This step is VITAL."
      ],
      "metadata": {
        "id": "DX2IHwrHdc9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_imdb_data(dataset_dir):\n",
        "    data = []\n",
        "    for label in ['pos', 'neg']:\n",
        "        label_dir = os.path.join(dataset_dir, label)\n",
        "        for filename in os.listdir(label_dir):\n",
        "            with open(os.path.join(label_dir, filename), 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "                tokens = word_tokenize(text.lower())\n",
        "                data.append(tokens)\n",
        "    return data\n",
        "\n",
        "dataset_dir = \"aclImdb/train\"\n",
        "preprocessed_data = preprocess_imdb_data(dataset_dir)"
      ],
      "metadata": {
        "id": "JGWV2F03WtXb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we pass in our preprocessed data and build our word2vec model for embeddings, consider the parameters in this model - sentences, vector_size, window, min_count, and workers."
      ],
      "metadata": {
        "id": "gXk6QkLydfMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* \tsentences: This parameter specifies the input data for training the Word2Vec model. In this case, preprocessed_data contains the preprocessed text data, where each element represents a list of tokens (words) from a movie review.\n",
        "* \t\tvector_size: This parameter determines the dimensionality of the word vectors (embeddings) produced by the Word2Vec model. In this example, vector_size=100 means that each word will be represented by a dense vector of 100 dimensions.\n",
        "* \t\twindow: This parameter sets the maximum distance between the current word being processed and the other words in the window. It defines the context window size for the model to consider when learning word embeddings. In this case, window=5 means that the model will consider the five words before and after the current word in the text.\n",
        "* \t\tmin_count: This parameter specifies the minimum frequency count of words required to be included in the vocabulary. Words with frequency counts lower than this value will be ignored and not considered for training. Setting min_count=1 means that all words present in the dataset will be included in the vocabulary.\n",
        "* \t\tworkers: This parameter sets the number of threads to use for training the Word2Vec model. It determines the parallelism during training. In this case, workers=4 means that four worker threads will be used for training the model, which can speed up the training process on multi-core machines.\n"
      ],
      "metadata": {
        "id": "upCVW0rofQEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=preprocessed_data, vector_size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "5LJA9bLZbBol"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We generate our word embeddings and store it into word_embeddings.\n",
        "Here, we look at the example of the word good. You can try any words from the data set!"
      ],
      "metadata": {
        "id": "FGPoupqpdnbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate word embeddings\n",
        "word_embeddings = model.wv\n",
        "\n",
        "# Example of how to use word embeddings\n",
        "word_vector = word_embeddings['good']\n",
        "print(\"Word embedding for 'good':\", word_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkVN3P8_bFGi",
        "outputId": "5da2ef95-a180-4c61-d4dd-d908e9941da2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word embedding for 'good': [ 2.309283    3.0937796  -0.3786237   0.23520526  1.6532553   0.10221997\n",
            " -0.13742884  0.9272892   0.99913126  1.0752318   1.7283039   1.4420184\n",
            " -1.510665   -2.4869087   1.9908297  -0.57715434  2.574168   -0.7567778\n",
            "  2.9907923  -1.138794   -0.8542557   3.1950972   0.1723586  -1.0455253\n",
            " -0.60189193 -2.4403648  -3.1632488   0.4443021   3.2067566  -1.1371427\n",
            " -1.2953486  -0.7832388   3.6576543   4.6164403  -0.03829485  0.905741\n",
            "  3.158804    0.33012706 -2.7263613   0.65177727 -1.6560317  -1.5961897\n",
            " -2.432053    0.04080295 -1.3640958  -1.4595801  -2.3624341   0.6443799\n",
            "  1.1676904   1.9009656  -2.0662339  -1.4817203  -0.65511596  4.0773234\n",
            "  1.3304667  -1.0425212   0.6225425   0.27257732  2.5247307   0.12556818\n",
            " -0.28358814 -1.6507988   1.524475    0.7140454  -0.45654103  1.97686\n",
            " -0.30895022  0.9721136   0.04969786 -2.393618   -0.45060483  1.6070594\n",
            " -1.6238753   1.9371495  -0.7520865  -1.3773664  -2.2521145  -2.135156\n",
            " -1.9033598  -2.4243119  -0.68769217  0.79146045  0.39681676  1.6260322\n",
            " -1.3493454  -1.0853728  -0.37093294  0.0497559  -2.3537214   1.5050887\n",
            "  2.0819442  -0.844099    2.4265773  -0.43065897 -1.7106451  -0.75058824\n",
            " -0.05646323  2.122016   -1.4731354  -1.865601  ]\n"
          ]
        }
      ]
    }
  ]
}